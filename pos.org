* links
** http://honnibal.wordpress.com/2013/12/18/a-simple-fast-algorithm-for-natural-language-dependency-parsing/
** https://gist.github.com/syllog1sm/10343947
** http://honnibal.wordpress.com/2013/09/11/a-good-part-of-speechpos-tagger-in-about-200-lines-of-python/

* Greedy Averaged Perceptron
** Averaged Perceptron
*** perceptron is bad at multi-tagging; for that, really want a prob distro
*** we just use a 'greedy' search
*** our training data will model the fact that our previous tags will be imperfect
** two tags of history
** features derived from Brown word clusters
*** http://metaoptimize.com/projects/wordreprs/
** case-insensitive features - robustness
** for efficiency - figure out which freq words (50%) have unambiguous tags
** no search strategy - just a greedy model

* supervised learning
** given table of data
** at runtime, the last column's data will be missing
** find correlations from the prev cols to predict last
** features: non-last ('predictor') columns
*** PoS at word i-1
*** word i-1 = 'Parliament'
*** last 3 letters at i+1
** prediction: last column
*** PoS at word i

* predicting (at runtime)
** py code
def predict(self, features):
    '''Dot-product the features and current weights and return the best class.'''
    scores = defaultdict(float)
    for feat in features:
        if feat not in self.weights:
            continue
        weights = self.weights[feat]
        for clas, weight in weights.items():
            scores[clas] += weight
    # Do a secondary alphabetic sort, for stability
    return max(self.classes, key=lambda clas: (scores[clas], clas))

** tables are sparse; use maps
** so rather than "weight vectors", we use "weight maps"
** input data (features)
*** set w/ a member for every non-0 'column' in our 'table' - every active feature
*** can be map or vector
**** map: set values for the features
**** vector: bools (present-or-absent)
** weights (pre-learned)
*** map of maps
*** feature -> class (i.e. pos) -> weight
*** don't do class -> feature -> weight
**** word freqs have Zipf's distribution
***** most words are rare
***** frequent words are very frequent

* learning the weights
** simple learning algorithm
*** when you make a mistake, modify weights
**** boost those for the correct class
**** penalize those that led to your false prediction
** steps
*** Receive a new pair: (features, PoS-tag)
*** Given the current 'weights' for the features, guess the value of the PoS tag
*** If wrong:
**** incr the weights associated with the correct class for these features
**** decr the weights for the predicted class
** code
def train(self, nr_iter, examples):
    for i in range(nr_iter):
        for features, true_tag in examples:
            guess = self.predict(features)
            if guess != true_tag:
                for f in features:
                    self.weights[f][true_tag] += 1
                    self.weights[f][guess] -= 1
        random.shuffle(examples)
*** nr_iter: number of iterations (to run training)
*** self.weights: map: feature -> class (i.e. pos) -> weight
*** do not converge
**** convergence: when no longer makes wrong guesses
**** do NOT want.  overfits.  how?
***** later iterations, pays too much attention to the few examples it gets wrong
***** mutates its whole model around them
*** instead, make weights more sticky
**** give the model less chance to ruin all its previous hard work
**** return the *averaged* weights, not the final weights
** averaging weights
*** Get the average weight assigned to a feature/class pair during learning
*** key component: total wt (acc) it was assigned
**** accumulator (don't store all intermediate vals)
**** divide acc by the num of iters at the end
*** lots of calculation
**** why?
***** Don't want to ave only after outer-loop iters.
***** Want the ave of all the values - from the inner loop.
***** W/ 5,000 examples & 10 training iters -> ave of 50,000 vals for each wt.
***** careful calculating acc - almost any instance, see only a tiny fraction of active feature/class pairs
**** how?
***** maintain another map - track how long each wt has gone unchanged
***** when do change a wt, do a fast-forward update to the acc (for all those iters where it lay unchanged)
*** update code
def update(self, truth, guess, features):
    def upd_feat(c, f, v):
        nr_iters_at_this_weight = self.i - self._timestamps[f][c]
        self._totals[f][c] += nr_iters_at_this_weight * self.weights[f][c]
        self.weights[f][c] += v
        self._timestamps[f][c] = self.i
 
    self.i += 1
    for f in features:
        upd_feat(truth, f, 1.0)
        upd_feat(guess, f, -1.0)
**** upd_feat(pos, feature, amount)
**** self.i: the iteration number
**** self._timestamps[feature][pos]: the last iter at which we saw this feature/pos pair
**** self._totals[feature][pos]: the running total of weights for this feature/pos pair

* features
** code
def _get_features(self, i, word, context, prev, prev2):
    '''Map tokens-in-contexts into a feature representation, implemented as a
    set. If the features change, a new model must be trained.'''
    def add(name, *args):
        features.add('+'.join((name,) + tuple(args)))
 
    features = set()
    add('bias') # This acts sort of like a prior
    add('i suffix', word[-3:])
    add('i pref1', word[0])
    add('i-1 tag', prev)
    add('i-2 tag', prev2)
    add('i tag+i-2 tag', prev, prev2)
    add('i word', context[i])
    add('i-1 tag+i word', prev, context[i])
    add('i-1 word', context[i-1])
    add('i-1 suffix', context[i-1][-3:])
    add('i-2 word', context[i-2])
    add('i+1 word', context[i+1])
    add('i+1 suffix', context[i+1][-3:])
    add('i+2 word', context[i+2])
    return features
** call this fn for each word
** i: the idx of the word
** word: same as context[i]
** context: list of all words
** prev, prev2: the two previous TAGs
** return a set of strings(?), where each element has a name (descr)
** 

* training loop
** code
def train(self, sentences, save_loc=None, nr_iter=5, quiet=False):
    '''Train a model from sentences, and save it at save_loc. nr_iter
    controls the number of Perceptron training iterations.'''
    self._make_tagdict(sentences, quiet=quiet)
    self.model.classes = self.classes
    prev, prev2 = START
    for iter_ in range(nr_iter):
        c = 0; n = 0
        for words, tags in sentences:
            context = START + [self._normalize(w) for w in words] + END
            for i, word in enumerate(words):
                guess = self.tagdict.get(word)
                if not guess:
                    feats = self._get_features(i, word, context, prev, prev2)
                    guess = self.model.predict(feats)
                    self.model.update(tags[i], guess, feats)
                # Set the history features from the guesses, not the true tags
                prev2 = prev; prev = guess
                c += guess == tags[i]; n += 1
        random.shuffle(sentences)
        if not quiet:
            print("Iter %d: %d/%d=%.3f" % (iter_, c, n, _pc(c, n)))
    self.model.average_weights()
    # Pickle as a binary file
    if save_loc is not None:
        cPickle.dump((self.model.weights, self.tagdict, self.classes),
                     open(save_loc, 'wb'), -1)



* optimizations
** no reason to divide weights.
*** the normalizing divisor is a constant.
*** just use the sums.
** weights
*** map: feature -> [(pos, weight)]
**** can iter per feature, getting sparse array
**** need every value returned; don't need to look 'em up

